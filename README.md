# TrainScale

> **A Scalable SOTA PyTorch Training Framework** â€” Above-Unsloth-level capabilities with 100% YAML-driven configuration.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## âœ¨ Features

### ğŸš€ End-to-End Pipeline
- **Zero Hardcoding** â€” All settings from YAML configuration
- **Auto-Discovery** â€” Automatic dataset introspection (splits, columns, schemas)
- **SOTA Preprocessing** â€” Token-aware content distribution, smart truncation
- **Production DataLoader** â€” Loss-aligned tensors ready for `model.forward()`

### ğŸ¯ Training Modes
| Mode | Description | VRAM Usage |
|------|-------------|------------|
| **Full Fine-tuning** | 16-bit/32-bit training | 100% |
| **LoRA** | Low-rank adaptation | ~60% |
| **QLoRA** | 4-bit NF4 quantization | ~30% |
| **FP8** | H100/L40 optimized | ~50% |
| **Pretraining** | From scratch | 100% |

### âš¡ SOTA Optimizers
- `AdamW` â€” Standard with weight decay
- `Adam8bit` â€” 8-bit Adam with dynamic quantization
- `Lion` â€” Google's sign-momentum optimizer (2x faster)
- `CAME` â€” Communication-efficient distributed
- `SophiaG` â€” Second-order Hessian optimizer
- `Prodigy` â€” Adaptive learning rate (no LR tuning needed)

### ğŸ“Š SOTA Schedulers
- `Cosine` â€” Cosine annealing with warmup
- `WSD` â€” LLaMA-3 Warmup-Stable-Decay
- `REX` â€” Rapid warmup + exponential decay
- `OneCycle` â€” Super-convergence scheduler
- `Polynomial` â€” Configurable power decay
- `CosineRestart` â€” SGDR with warm restarts

### ğŸ² RL Training (80% VRAM Reduction)
- **GRPO** â€” Group Relative Policy Optimization
- **DrGRPO** â€” Dynamic Reward GRPO
- **DAPO** â€” Decoupled Advantage Policy Optimization
- **PPO** â€” Proximal Policy Optimization
- **DPO/ORPO/SimPO** â€” Preference optimization

---

## ğŸ› ï¸ Installation

```bash
# Clone repository
git clone https://github.com/yourusername/TrainScale.git
cd TrainScale

# Install dependencies
pip install -r requirements.txt

# Optional: Flash Attention 2
pip install flash-attn --no-build-isolation
```

---

## ğŸš€ Quick Start

### 1. Run E2E Pipeline Demo

```bash
# Quick test with small sample
python data_pipeline/examples/e2e_complete_demo.py \
    --config data_pipeline/examples/test_pipeline.yaml

# Full production run
python data_pipeline/examples/e2e_complete_demo.py \
    --config data_pipeline/examples/complete_pipeline.yaml
```

### 2. YAML Configuration

All settings are controlled via YAML â€” **no hardcoding**:

```yaml
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Dataset Configuration
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
dataset:
  name: "tatsu-lab/alpaca"
  splits:
    train:
      name: "train"
      sample_size: 1000  # null = all data
      shuffle: true
      seed: 42

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Tokenizer Configuration
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
tokenizer:
  name_or_path: "meta-llama/Llama-3.1-8B-Instruct"
  max_length: 4096
  padding: "max_length"
  truncation: true

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Prompt Template (Jinja2 Supported)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
prompt_template:
  format_type: "custom"  # chat, completion, custom
  template: |
    ### Instruction:
    {{ instruction }}
    {% if input %}
    ### Input:
    {{ input }}
    {% endif %}
    ### Response:
    {{ output }}
  input_columns: ["instruction", "input"]
  label_column: "output"
  mask_input: true
```

---

## ğŸ”§ Hardware & Token Settings

### Recommended Settings by GPU

| GPU | VRAM | Max Length | Batch Size | Precision | Mode |
|-----|------|------------|------------|-----------|------|
| **RTX 3090** | 24GB | 2048 | 4 | bf16 | QLoRA |
| **RTX 4090** | 24GB | 4096 | 4 | bf16 | QLoRA |
| **A100 40GB** | 40GB | 8192 | 8 | bf16 | LoRA |
| **A100 80GB** | 80GB | 8192 | 16 | bf16 | Full |
| **H100** | 80GB | 16384 | 32 | fp8 | Full |
| **L40** | 48GB | 8192 | 12 | fp8 | Full |

### YAML Hardware Configuration

```yaml
training:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Hardware Settings
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  hardware:
    device: "auto"          # auto, cuda, cuda:0, cpu
    precision: "bf16"       # bf16, fp16, fp32, fp8
    tf32: true              # Enable TF32 (Ampere+)
    compile_model: false    # torch.compile (PyTorch 2.0+)
  
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Memory Optimization
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  kernels:
    use_triton: true              # Triton kernels
    use_flash_attention: true     # Flash Attention 2
    use_fused_cross_entropy: true # Fused CE loss
    activation_checkpointing: true # Gradient checkpointing
```

---

## ğŸ“Š Token & Sequence Settings

### Preprocessing Configuration

```yaml
preprocessing:
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Length Manager: Per-column limits and truncation
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  length_manager:
    enabled: true
    max_total_length: 4096        # Maximum sequence length
    padding_strategy: "longest"   # longest, max_length, do_not_pad, bucket
    truncation_strategy: "smart"  # smart, simple, word_boundary, sentence_boundary
    
    # Per-column character limits
    per_column_limits:
      instruction: 4000   # Max chars for instruction
      input: 4000         # Max chars for input
      output: 8000        # Max chars for output

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Content Distribution: Token-aware allocation
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  content_distribution:
    enabled: true
    mode: "proportional"  # equal, proportional, ratio, priority, adaptive
    column_ratios:
      instruction: 0.3
      input: 0.1
      output: 0.55
    special_tokens_budget: 10
```

### Truncation Strategies

| Strategy | Description | Use Case |
|----------|-------------|----------|
| `smart` | Prefers sentence > word > simple | General purpose |
| `sentence_boundary` | Truncate at sentence end | Preserves semantics |
| `word_boundary` | Truncate at word boundary | Avoids mid-word cuts |
| `simple` | Hard cut at limit | Maximum content |

### Padding Strategies

| Strategy | Description | Use Case |
|----------|-------------|----------|
| `longest` | Pad to longest in batch | Memory efficient |
| `max_length` | Pad to fixed max_length | Consistent shapes |
| `bucket` | Bucket by length (128,256,512...) | Best efficiency |
| `do_not_pad` | No padding (for packing) | Sequence packing |

---

## ğŸ® CLI Arguments

```bash
python data_pipeline/examples/e2e_complete_demo.py \
    --config <path>         # YAML config file (required)
    --split <name>          # Split to use (default: train)
    --train                 # Enable training mode
    --output-dir <path>     # Output directory
```

### Example Commands

```bash
# Quick test (100 samples, GPT-2 tokenizer)
python data_pipeline/examples/e2e_complete_demo.py \
    --config data_pipeline/examples/test_pipeline.yaml

# Full training with Llama-3.1
python data_pipeline/examples/e2e_complete_demo.py \
    --config data_pipeline/examples/complete_pipeline.yaml \
    --train \
    --output-dir ./outputs

# Validation split only
python data_pipeline/examples/e2e_complete_demo.py \
    --config my_config.yaml \
    --split validation
```

---

## ğŸ“ Project Structure

```
TrainScale/
â”œâ”€â”€ data_pipeline/
â”‚   â”œâ”€â”€ core/               # Config schema, types, errors
â”‚   â”œâ”€â”€ data/               # DataLoader, collate functions
â”‚   â”œâ”€â”€ introspection/      # Dataset discovery
â”‚   â”œâ”€â”€ preprocessing/      # Tokenization, prompts, length management
â”‚   â”œâ”€â”€ trainer/            # SOTA trainer, optimizers, schedulers
â”‚   â”‚   â”œâ”€â”€ core/           # SOTAConfig, base trainer
â”‚   â”‚   â”œâ”€â”€ optimizers/     # Adam8bit, Lion, CAME, SophiaG
â”‚   â”‚   â”œâ”€â”€ schedulers/     # WSD, REX, OneCycle, CosineRestart
â”‚   â”‚   â”œâ”€â”€ loss/           # Chunked CE, DPO, ORPO, SimPO
â”‚   â”‚   â””â”€â”€ kernels/        # Triton kernels, Flash Attention
â”‚   â””â”€â”€ examples/           # Demo configs and scripts
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§ª Verification

Run the test suite to verify installation:

```bash
# Test imports
python -c "from data_pipeline.trainer import SOTAConfig, SOTATrainer; print('âœ… Imports OK')"

# Test E2E pipeline
python data_pipeline/examples/e2e_complete_demo.py \
    --config data_pipeline/examples/test_pipeline.yaml
```

Expected output:
```
============================================================
PIPELINE SUMMARY
============================================================
Config: data_pipeline/examples/test_pipeline.yaml
Discovered splits: ['train']
Discovered columns: ['instruction', 'input', 'output', 'text']

Batch Output:
  input_ids: torch.Size([4, 512]) (torch.int64)
  attention_mask: torch.Size([4, 512])
  labels: torch.Size([4, 512])

âœ… Pipeline complete! Tensors are ready for model.forward()
```

---

## ğŸ“š Documentation

- [Complete Pipeline YAML](data_pipeline/examples/complete_pipeline.yaml) â€” Full production config
- [Test Pipeline YAML](data_pipeline/examples/test_pipeline.yaml) â€” Quick testing config
- [SOTA Config Schema](data_pipeline/trainer/core/sota_config.py) â€” All training options

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

MIT License â€” see [LICENSE](LICENSE) for details.

---

## â­ Acknowledgments

TrainScale builds upon excellent open-source projects:
- [HuggingFace Transformers](https://github.com/huggingface/transformers)
- [PyTorch](https://pytorch.org/)
- [Flash Attention](https://github.com/Dao-AILab/flash-attention)
- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)

---

<p align="center">
  <b>TrainScale</b> â€” Train Smarter, Scale Faster ğŸš€
</p>
