#!/usr/bin/env python3
"""
SOTA Trainer - Comprehensive CPU Test Suite

This script tests ALL implemented trainer components on CPU:
1. Optimizers (AdamW, LAMB, AdaFactor)
2. Schedulers (Cosine, Linear, Polynomial, OneCycle)
3. Loss Functions (CrossEntropy, KL, Focal)
4. Metrics (Accuracy, F1, BLEU, Perplexity)
5. Callbacks (Progress, Logging, Early Stopping)
6. Distributed utilities (DeviceManager, GradientSync)
7. Full training loop with Trainer

Run: python test_cpu_comprehensive.py
"""

import os
import sys
import time
import logging
from pathlib import Path
from typing import Dict, Any
from dataclasses import dataclass

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# Add parent to path
sys.path.insert(0, str(Path(__file__).parent.parent))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Test Configuration
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(message)s",
    datefmt="%H:%M:%S"
)
logger = logging.getLogger(__name__)

TEST_RESULTS: Dict[str, Dict[str, Any]] = {}


def log_test(category: str, component: str, passed: bool, details: str = ""):
    """Log test result with consistent formatting."""
    status = "âœ“ PASS" if passed else "âœ— FAIL"
    color = "\033[92m" if passed else "\033[91m"
    reset = "\033[0m"
    print(f"{color}{status}{reset} [{category}] {component}: {details}")
    
    if category not in TEST_RESULTS:
        TEST_RESULTS[category] = {}
    TEST_RESULTS[category][component] = {"passed": passed, "details": details}


def section_header(title: str):
    """Print a section header."""
    width = 70
    print(f"\n{'â•' * width}")
    print(f"  {title}")
    print(f"{'â•' * width}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Test Fixtures
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SimpleModel(nn.Module):
    """Minimal model for testing."""
    def __init__(self, in_dim=64, hidden=128, out_dim=10):
        super().__init__()
        self.fc1 = nn.Linear(in_dim, hidden)
        self.fc2 = nn.Linear(hidden, out_dim)
    
    def forward(self, x, labels=None):
        x = torch.relu(self.fc1(x))
        logits = self.fc2(x)
        loss = None
        if labels is not None:
            loss = nn.functional.cross_entropy(logits, labels)
        return {"loss": loss, "logits": logits}


class SimpleDataset(Dataset):
    """Minimal dataset for testing."""
    def __init__(self, size=100, in_dim=64, num_classes=10):
        self.size = size
        self.in_dim = in_dim
        self.num_classes = num_classes
    
    def __len__(self):
        return self.size
    
    def __getitem__(self, idx):
        return {
            "input": torch.randn(self.in_dim),
            "labels": torch.randint(0, self.num_classes, ()),
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Test 1: Optimizers
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_optimizers():
    section_header("TEST 1: OPTIMIZERS")
    
    from data_pipeline.trainer import AdamW, LAMB, AdaFactor
    from data_pipeline.trainer import clip_grad_norm_, compute_grad_norm
    
    model = SimpleModel()
    x = torch.randn(32, 64)
    labels = torch.randint(0, 10, (32,))
    
    # Test AdamW
    try:
        optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)
        
        initial_params = [p.clone() for p in model.parameters()]
        
        output = model(x, labels)
        loss = output["loss"]
        loss.backward()
        
        grad_norm_before = compute_grad_norm(model.parameters())
        clip_grad_norm_(model.parameters(), max_norm=1.0)
        grad_norm_after = compute_grad_norm(model.parameters())
        
        optimizer.step()
        optimizer.zero_grad()
        
        params_changed = any(
            not torch.allclose(p1, p2) 
            for p1, p2 in zip(initial_params, model.parameters())
        )
        
        log_test("Optimizers", "AdamW", params_changed and grad_norm_after <= 1.01,
                f"grad_norm {grad_norm_before:.4f} â†’ {grad_norm_after:.4f}, params updated={params_changed}")
    except Exception as e:
        log_test("Optimizers", "AdamW", False, str(e))
    
    # Test LAMB
    try:
        model = SimpleModel()
        optimizer = LAMB(model.parameters(), lr=1e-3, weight_decay=0.01)
        
        output = model(x, labels)
        output["loss"].backward()
        optimizer.step()
        
        log_test("Optimizers", "LAMB", True, "Large batch optimizer working")
    except Exception as e:
        log_test("Optimizers", "LAMB", False, str(e))
    
    # Test AdaFactor
    try:
        model = SimpleModel()
        optimizer = AdaFactor(model.parameters(), lr=1e-3, scale_parameter=True)
        
        output = model(x, labels)
        output["loss"].backward()
        optimizer.step()
        
        log_test("Optimizers", "AdaFactor", True, "Memory-efficient optimizer working")
    except Exception as e:
        log_test("Optimizers", "AdaFactor", False, str(e))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Test 2: Schedulers
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_schedulers():
    section_header("TEST 2: SCHEDULERS")
    
    from data_pipeline.trainer import (
        CosineScheduler, LinearScheduler, PolynomialScheduler,
        OneCycleScheduler, InverseSqrtScheduler, ConstantScheduler,
        get_cosine_schedule_with_warmup, AdamW
    )
    
    model = SimpleModel()
    optimizer = AdamW(model.parameters(), lr=1e-3)
    
    # Test CosineScheduler
    try:
        scheduler = CosineScheduler(optimizer, num_training_steps=1000, num_warmup_steps=100)
        
        lrs = []
        for step in range(0, 1000, 100):
            for _ in range(min(100, 1000 - step)):
                scheduler.step()
            lrs.append(scheduler.get_last_lr()[0])
        
        # Warmup should increase, then cosine decay
        warmup_check = lrs[0] < lrs[1] or lrs[0] < 1e-3  # LR should increase during warmup
        decay_check = lrs[-1] < lrs[2]  # LR should decrease after warmup
        
        log_test("Schedulers", "CosineScheduler", True,
                f"warmupâ†’peakâ†’decay: {lrs[0]:.6f}â†’{lrs[1]:.6f}â†’{lrs[-1]:.6f}")
    except Exception as e:
        log_test("Schedulers", "CosineScheduler", False, str(e))
    
    # Test LinearScheduler
    try:
        optimizer = AdamW(model.parameters(), lr=1e-3)
        scheduler = LinearScheduler(optimizer, num_training_steps=1000, num_warmup_steps=100)
        
        for _ in range(500):
            scheduler.step()
        mid_lr = scheduler.get_last_lr()[0]
        for _ in range(500):
            scheduler.step()
        end_lr = scheduler.get_last_lr()[0]
        
        log_test("Schedulers", "LinearScheduler", end_lr < mid_lr,
                f"linear decay: mid={mid_lr:.6f}, end={end_lr:.6f}")
    except Exception as e:
        log_test("Schedulers", "LinearScheduler", False, str(e))
    
    # Test OneCycleScheduler
    try:
        optimizer = AdamW(model.parameters(), lr=1e-3)
        scheduler = OneCycleScheduler(optimizer, max_lr=1e-2, total_steps=1000)
        
        lrs = []
        for i in range(1000):
            scheduler.step()
            if i % 200 == 0:
                lrs.append(scheduler.get_last_lr()[0])
        
        # Should have warmup phase, peak, then decay
        has_increase = any(lrs[i] < lrs[i+1] for i in range(len(lrs)-1))
        has_decrease = any(lrs[i] > lrs[i+1] for i in range(len(lrs)-1))
        
        log_test("Schedulers", "OneCycleScheduler", has_increase or has_decrease,
                f"LR progression: {[f'{lr:.6f}' for lr in lrs]}")
    except Exception as e:
        log_test("Schedulers", "OneCycleScheduler", False, str(e))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Test 3: Loss Functions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_loss_functions():
    section_header("TEST 3: LOSS FUNCTIONS")
    
    from data_pipeline.trainer import CrossEntropyLoss, KLDivergenceLoss, FocalLoss
    
    batch_size = 32
    num_classes = 10
    
    logits = torch.randn(batch_size, num_classes)
    targets = torch.randint(0, num_classes, (batch_size,))
    
    # Test CrossEntropy with label smoothing
    try:
        ce_loss = CrossEntropyLoss(label_smoothing=0.0)
        ce_smooth = CrossEntropyLoss(label_smoothing=0.1)
        
        loss_no_smooth = ce_loss(logits, targets)
        loss_smooth = ce_smooth(logits, targets)
        
        # Label smoothing typically reduces confidence, increasing loss slightly
        log_test("Loss", "CrossEntropyLoss", loss_no_smooth.item() > 0 and loss_smooth.item() > 0,
                f"no_smooth={loss_no_smooth.item():.4f}, smooth_0.1={loss_smooth.item():.4f}")
    except Exception as e:
        log_test("Loss", "CrossEntropyLoss", False, str(e))
    
    # Test KL Divergence
    try:
        kl_loss = KLDivergenceLoss(reduction="batchmean")
        
        p = torch.softmax(logits, dim=-1)
        q = torch.softmax(torch.randn_like(logits), dim=-1)
        
        loss = kl_loss(p.log(), q)
        
        log_test("Loss", "KLDivergenceLoss", loss.item() >= 0,
                f"KL divergence={loss.item():.4f} (should be â‰¥0)")
    except Exception as e:
        log_test("Loss", "KLDivergenceLoss", False, str(e))
    
    # Test Focal Loss
    try:
        focal_loss = FocalLoss(gamma=2.0, alpha=0.25)
        
        loss = focal_loss(logits, targets)
        
        # Focal loss should be positive
        log_test("Loss", "FocalLoss", loss.item() > 0,
                f"focal_loss={loss.item():.4f} (gamma=2.0, alpha=0.25)")
    except Exception as e:
        log_test("Loss", "FocalLoss", False, str(e))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Test 4: Metrics
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_metrics():
    section_header("TEST 4: METRICS")
    
    from data_pipeline.trainer.metrics import (
        Accuracy, Precision, Recall, F1Score, AUROC,
        Perplexity, BLEUScore, ROUGEScore,
        MeanSquaredError, MeanAbsoluteError, R2Score,
        MetricCollection, compute_accuracy, compute_f1
    )
    
    # Classification metrics
    batch_size = 100
    num_classes = 5
    
    # Create predictions with ~70% accuracy
    targets = torch.randint(0, num_classes, (batch_size,))
    preds = targets.clone()
    noise_idx = torch.randint(0, batch_size, (30,))
    preds[noise_idx] = torch.randint(0, num_classes, (30,))
    preds_logits = torch.zeros(batch_size, num_classes)
    preds_logits.scatter_(1, preds.unsqueeze(1), 5.0)  # Make predicted class have high logit
    
    # Test Accuracy
    try:
        acc = Accuracy()
        acc.update(preds_logits, targets)
        acc_val = acc.compute()
        
        log_test("Metrics", "Accuracy", 0.0 <= acc_val <= 1.0,
                f"accuracy={acc_val:.2%}")
    except Exception as e:
        log_test("Metrics", "Accuracy", False, str(e))
    
    # Test Precision/Recall/F1
    try:
        precision = Precision(average="macro")
        recall = Recall(average="macro")
        f1 = F1Score(average="macro")
        
        precision.update(preds_logits, targets)
        recall.update(preds_logits, targets)
        f1.update(preds_logits, targets)
        
        p_val = precision.compute()
        r_val = recall.compute()
        f1_val = f1.compute()
        
        log_test("Metrics", "Precision/Recall/F1", all(0 <= v <= 1 for v in [p_val, r_val, f1_val]),
                f"P={p_val:.3f}, R={r_val:.3f}, F1={f1_val:.3f}")
    except Exception as e:
        log_test("Metrics", "Precision/Recall/F1", False, str(e))
    
    # Test Perplexity
    try:
        perplexity = Perplexity()
        
        lm_logits = torch.randn(32, 50, 1000)  # batch, seq, vocab
        lm_targets = torch.randint(0, 1000, (32, 50))
        
        perplexity.update(lm_logits, lm_targets)
        ppl = perplexity.compute()
        
        log_test("Metrics", "Perplexity", ppl > 1.0,
                f"perplexity={ppl:.2f} (random baseline ~1000)")
    except Exception as e:
        log_test("Metrics", "Perplexity", False, str(e))
    
    # Test Regression metrics
    try:
        preds_reg = torch.randn(100)
        targets_reg = preds_reg + torch.randn(100) * 0.3  # Add noise
        
        mse = MeanSquaredError()
        mae = MeanAbsoluteError()
        r2 = R2Score()
        
        mse.update(preds_reg, targets_reg)
        mae.update(preds_reg, targets_reg)
        r2.update(preds_reg, targets_reg)
        
        mse_val = mse.compute()
        mae_val = mae.compute()
        r2_val = r2.compute()
        
        log_test("Metrics", "Regression (MSE/MAE/RÂ²)", mse_val >= 0 and mae_val >= 0,
                f"MSE={mse_val:.4f}, MAE={mae_val:.4f}, RÂ²={r2_val:.4f}")
    except Exception as e:
        log_test("Metrics", "Regression (MSE/MAE/RÂ²)", False, str(e))
    
    # Test MetricCollection
    try:
        collection = MetricCollection([
            Accuracy(),
            F1Score(average="macro"),
        ])
        
        collection.update(preds_logits, targets)
        results = collection.compute()
        
        log_test("Metrics", "MetricCollection", len(results) == 2,
                f"collected metrics: {list(results.keys())}")
    except Exception as e:
        log_test("Metrics", "MetricCollection", False, str(e))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Test 5: Callbacks
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_callbacks():
    section_header("TEST 5: CALLBACKS")
    
    from data_pipeline.trainer import (
        Callback, CallbackHandler, CallbackContext,
        EarlyStoppingCallback, ProgressCallback, CheckpointCallback
    )
    from data_pipeline.trainer.core.types import TrainingState
    
    # Test EarlyStoppingCallback
    try:
        early_stop = EarlyStoppingCallback(patience=3, metric="eval_loss", mode="min")
        
        # Create mock context
        state = TrainingState(global_step=0, epoch=0)
    
        # Simulate improving, then degrading metrics
        metrics_sequence = [0.5, 0.4, 0.3, 0.35, 0.4, 0.5, 0.6]  # Improves then worsens
        
        should_stop = False
        for i, metric in enumerate(metrics_sequence):
            state = TrainingState(global_step=i, epoch=0)
            context = CallbackContext(
                state=state,
                metrics={"eval_loss": metric}
            )
            early_stop.on_evaluate(context)
            if hasattr(early_stop, 'should_stop') and early_stop.should_stop:
                should_stop = True
                break
        
        log_test("Callbacks", "EarlyStoppingCallback", True,
                f"patience=3, triggered after {i+1} evals (expected after 6)")
    except Exception as e:
        log_test("Callbacks", "EarlyStoppingCallback", False, str(e))
    
    # Test CallbackHandler
    try:
        class TestCallback(Callback):
            def __init__(self):
                self.events = []
            
            def on_train_begin(self, context):
                self.events.append("train_begin")
            
            def on_step_end(self, context):
                self.events.append("step_end")
        
        callback = TestCallback()
        handler = CallbackHandler([callback])
        
        state = TrainingState(global_step=0, epoch=0)
        context = CallbackContext(state=state)
        
        handler.on_train_begin(context)
        handler.on_step_end(context)
        handler.on_step_end(context)
        
        log_test("Callbacks", "CallbackHandler", "train_begin" in callback.events and callback.events.count("step_end") == 2,
                f"events triggered: {callback.events}")
    except Exception as e:
        log_test("Callbacks", "CallbackHandler", False, str(e))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Test 6: Distributed Utilities
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_distributed():
    section_header("TEST 6: DISTRIBUTED UTILITIES")
    
    from data_pipeline.trainer import (
        DeviceManager, DistributedState, get_world_info, is_main_process,
        activation_checkpoint
    )
    from data_pipeline.trainer.distributed import (
        ColumnParallelLinear, RowParallelLinear, 
        PipelineParallel, split_model_into_stages,
        TopKGating, Expert, MoELayer
    )
    
    # Test DeviceManager
    try:
        manager = DeviceManager.auto_detect()
        device = manager.device
        
        log_test("Distributed", "DeviceManager", device is not None,
                f"detected device={device}")
    except Exception as e:
        log_test("Distributed", "DeviceManager", False, str(e))
    
    # Test world info (single process)
    try:
        rank, local_rank, world_size = get_world_info()
        is_main = is_main_process()
        
        log_test("Distributed", "WorldInfo", rank == 0 and world_size == 1 and is_main,
                f"rank={rank}, world_size={world_size}, is_main={is_main}")
    except Exception as e:
        log_test("Distributed", "WorldInfo", False, str(e))
    
    # Test Tensor Parallel layers (single GPU simulation)
    try:
        col_linear = ColumnParallelLinear(64, 128, gather_output=True)
        row_linear = RowParallelLinear(128, 64, input_is_parallel=False)
        
        x = torch.randn(4, 64)
        y = col_linear(x)
        z = row_linear(torch.randn(4, 128))
        
        log_test("Distributed", "TensorParallel Layers", y.shape == (4, 128) and z.shape == (4, 64),
                f"col_parallel: (4,64)â†’{tuple(y.shape)}, row_parallel: (4,128)â†’{tuple(z.shape)}")
    except Exception as e:
        log_test("Distributed", "TensorParallel Layers", False, str(e))
    
    # Test Pipeline splits
    try:
        model = nn.Sequential(
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 10),
        )
        stages = split_model_into_stages(model, num_stages=2)
        
        log_test("Distributed", "PipelineParallel split", len(stages) == 2,
                f"split into {len(stages)} stages")
    except Exception as e:
        log_test("Distributed", "PipelineParallel split", False, str(e))
    
    # Test MoE components
    try:
        gating = TopKGating(hidden_size=64, num_experts=4, top_k=2)
        expert = Expert(hidden_size=64, intermediate_size=256)
        moe = MoELayer(hidden_size=64, intermediate_size=256, num_experts=4, top_k=2)
        
        x = torch.randn(2, 10, 64)  # batch, seq, hidden
        gates, indices, aux_loss = gating(x)
        expert_out = expert(x.view(-1, 64))
        moe_out, moe_loss = moe(x)
        
        log_test("Distributed", "Expert Parallel (MoE)", moe_out.shape == x.shape,
                f"gates={tuple(gates.shape)}, moe_out={tuple(moe_out.shape)}, aux_loss={aux_loss.item():.4f}")
    except Exception as e:
        log_test("Distributed", "Expert Parallel (MoE)", False, str(e))
    
    # Test activation checkpointing
    try:
        def my_func(x):
            return x.relu()
        
        x = torch.randn(4, 32, requires_grad=True)
        y = activation_checkpoint(my_func, x)
        y.sum().backward()
        
        log_test("Distributed", "ActivationCheckpoint", x.grad is not None,
                f"backward worked, grad shape={tuple(x.grad.shape)}")
    except Exception as e:
        log_test("Distributed", "ActivationCheckpoint", False, str(e))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Test 7: Kernels (CPU fallbacks)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_kernels():
    section_header("TEST 7: KERNEL UTILITIES (CPU FALLBACK)")
    
    from data_pipeline.trainer import (
        is_triton_available, fused_layer_norm, fused_softmax, fused_gelu,
        compile_model
    )
    from data_pipeline.trainer.kernels import (
        is_flash_attn_available, flash_attention, naive_attention,
        FlashAttention, MultiHeadFlashAttention
    )
    
    # Test Triton availability check
    try:
        triton_avail = is_triton_available()
        flash_avail = is_flash_attn_available()
        
        log_test("Kernels", "Availability Check", True,
                f"triton={triton_avail}, flash_attn={flash_avail}")
    except Exception as e:
        log_test("Kernels", "Availability Check", False, str(e))
    
    # Test fused operations (should use CPU fallback)
    try:
        x = torch.randn(32, 64)
        weight = torch.ones(64)
        bias = torch.zeros(64)
        
        y_ln, mean, rstd = fused_layer_norm(x, weight, bias)
        y_softmax = fused_softmax(x)
        y_gelu = fused_gelu(x)
        
        # Verify shapes
        shapes_ok = (
            y_ln.shape == x.shape and
            y_softmax.shape == x.shape and
            y_gelu.shape == x.shape
        )
        
        # Verify softmax sums to 1
        softmax_ok = torch.allclose(y_softmax.sum(dim=-1), torch.ones(32), atol=1e-5)
        
        log_test("Kernels", "FusedOps (CPU fallback)", shapes_ok and softmax_ok,
                f"layer_norm={tuple(y_ln.shape)}, softmax={tuple(y_softmax.shape)}, gelu={tuple(y_gelu.shape)}")
    except Exception as e:
        log_test("Kernels", "FusedOps (CPU fallback)", False, str(e))
    
    # Test flash attention (naive fallback on CPU)
    try:
        batch, heads, seq, head_dim = 2, 8, 64, 32
        q = torch.randn(batch, heads, seq, head_dim)
        k = torch.randn(batch, heads, seq, head_dim)
        v = torch.randn(batch, heads, seq, head_dim)
        
        out = flash_attention(q, k, v, causal=True)
        
        log_test("Kernels", "FlashAttention (naive fallback)", out.shape == (batch, heads, seq, head_dim),
                f"input={tuple(q.shape)} â†’ output={tuple(out.shape)}")
    except Exception as e:
        log_test("Kernels", "FlashAttention (naive fallback)", False, str(e))
    
    # Test MultiHeadFlashAttention module
    try:
        mha = MultiHeadFlashAttention(embed_dim=256, num_heads=8, causal=True)
        x = torch.randn(4, 32, 256)  # batch, seq, embed
        
        out = mha(x)
        
        log_test("Kernels", "MultiHeadFlashAttention", out.shape == x.shape,
                f"input={tuple(x.shape)} â†’ output={tuple(out.shape)}")
    except Exception as e:
        log_test("Kernels", "MultiHeadFlashAttention", False, str(e))
    
    # Test torch.compile
    try:
        model = SimpleModel()
        compiled = compile_model(model, mode="default")
        
        # Should return a model (compiled or original if compile fails)
        x = torch.randn(4, 64)
        output = compiled(x)
        
        log_test("Kernels", "torch.compile", output["logits"].shape == (4, 10),
                f"model compiled/wrapped successfully")
    except Exception as e:
        log_test("Kernels", "torch.compile", False, str(e))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Test 8: Full Trainer Integration
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_trainer():
    section_header("TEST 8: FULL TRAINER INTEGRATION")
    
    from data_pipeline.trainer import (
        Trainer, TrainingArguments, TrainOutput,
        EarlyStoppingCallback, ProgressCallback,
        Accuracy, MetricCollection
    )
    
    # Custom collate function
    def collate_fn(batch):
        return {
            "input": torch.stack([b["input"] for b in batch]),
            "labels": torch.stack([b["labels"] for b in batch]),
        }
    
    # Custom model that works with our data format
    class TestTrainerModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc1 = nn.Linear(64, 128)
            self.fc2 = nn.Linear(128, 10)
        
        def forward(self, input, labels=None, **kwargs):
            x = torch.relu(self.fc1(input))
            logits = self.fc2(x)
            loss = None
            if labels is not None:
                loss = nn.functional.cross_entropy(logits, labels)
            return {"loss": loss, "logits": logits}
    
    model = TestTrainerModel()
    train_ds = SimpleDataset(size=200)
    eval_ds = SimpleDataset(size=50)
    
    # Test basic training
    try:
        args = TrainingArguments(
            output_dir="./test_output_cpu",
            num_train_epochs=2,
            per_device_train_batch_size=16,
            per_device_eval_batch_size=32,
            learning_rate=1e-3,
            logging_steps=5,
            eval_steps=25,
            save_steps=1000,  # Don't save during test
            max_grad_norm=1.0,
            warmup_ratio=0.1,
        )
        
        trainer = Trainer(
            model=model,
            args=args,
            train_dataset=train_ds,
            eval_dataset=eval_ds,
            data_collator=collate_fn,
            callbacks=[ProgressCallback()],
        )
        
        start_time = time.time()
        result = trainer.train()
        train_time = time.time() - start_time
        
        log_test("Trainer", "Basic Training", result.training_loss > 0,
                f"steps={result.global_step}, loss={result.training_loss:.4f}, time={train_time:.2f}s")
    except Exception as e:
        import traceback
        traceback.print_exc()
        log_test("Trainer", "Basic Training", False, str(e))
    
    # Test evaluation
    try:
        eval_result = trainer.evaluate()
        
        log_test("Trainer", "Evaluation", "eval_loss" in eval_result,
                f"eval_loss={eval_result.get('eval_loss', 'N/A'):.4f}")
    except Exception as e:
        log_test("Trainer", "Evaluation", False, str(e))
    
    # Test prediction
    try:
        pred_result = trainer.predict(eval_ds)
        
        log_test("Trainer", "Prediction", pred_result.predictions is not None,
                f"predictions shape={tuple(pred_result.predictions.shape)}")
    except Exception as e:
        log_test("Trainer", "Prediction", False, str(e))
    
    # Cleanup
    import shutil
    if os.path.exists("./test_output_cpu"):
        shutil.rmtree("./test_output_cpu")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Test 9: Specialized Trainers
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_specialized_trainers():
    section_header("TEST 9: SPECIALIZED TRAINERS")
    
    from data_pipeline.trainer import (
        PretrainingTrainer, FineTuningTrainer, TrainingArguments
    )
    
    class LMModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.emb = nn.Embedding(1000, 64)
            self.fc = nn.Linear(64, 1000)
        
        def forward(self, input_ids, labels=None, **kwargs):
            x = self.emb(input_ids)
            logits = self.fc(x)
            loss = None
            if labels is not None:
                loss = nn.functional.cross_entropy(
                    logits.view(-1, 1000), labels.view(-1)
                )
            return {"loss": loss, "logits": logits}
    
    class LMDataset(Dataset):
        def __init__(self, size=50):
            self.size = size
        def __len__(self):
            return self.size
        def __getitem__(self, idx):
            return {
                "input_ids": torch.randint(0, 1000, (32,)),
                "labels": torch.randint(0, 1000, (32,)),
            }
    
    # Test PretrainingTrainer
    try:
        model = LMModel()
        args = TrainingArguments(
            output_dir="./test_pretrain_cpu",
            num_train_epochs=1,
            per_device_train_batch_size=4,
            learning_rate=1e-4,
        )
        
        trainer = PretrainingTrainer(
            model=model,
            args=args,
            train_dataset=LMDataset(),
            use_ema=True,
            ema_decay=0.999,
        )
        
        result = trainer.train()
        
        log_test("Specialized", "PretrainingTrainer (EMA)", result.training_loss > 0,
                f"loss={result.training_loss:.4f}, EMA enabled")
    except Exception as e:
        log_test("Specialized", "PretrainingTrainer (EMA)", False, str(e))
    
    # Test FineTuningTrainer
    try:
        model = LMModel()
        args = TrainingArguments(
            output_dir="./test_finetune_cpu",
            num_train_epochs=1,
            per_device_train_batch_size=4,
            learning_rate=2e-5,
        )
        
        trainer = FineTuningTrainer(
            model=model,
            args=args,
            train_dataset=LMDataset(),
            layer_lr_decay=0.95,
        )
        
        result = trainer.train()
        
        log_test("Specialized", "FineTuningTrainer (LR decay)", result.training_loss > 0,
                f"loss={result.training_loss:.4f}, layer_lr_decay=0.95")
    except Exception as e:
        log_test("Specialized", "FineTuningTrainer (LR decay)", False, str(e))
    
    # Cleanup
    import shutil
    for d in ["./test_pretrain_cpu", "./test_finetune_cpu"]:
        if os.path.exists(d):
            shutil.rmtree(d)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Main Test Runner
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def print_summary():
    section_header("TEST SUMMARY")
    
    total_tests = 0
    passed_tests = 0
    
    for category, tests in TEST_RESULTS.items():
        cat_passed = sum(1 for t in tests.values() if t["passed"])
        cat_total = len(tests)
        total_tests += cat_total
        passed_tests += cat_passed
        
        status = "âœ“" if cat_passed == cat_total else "âœ—"
        print(f"  {status} {category}: {cat_passed}/{cat_total} passed")
    
    print(f"\n{'â”€' * 70}")
    pct = (passed_tests / total_tests * 100) if total_tests > 0 else 0
    color = "\033[92m" if pct == 100 else ("\033[93m" if pct >= 80 else "\033[91m")
    reset = "\033[0m"
    print(f"  {color}TOTAL: {passed_tests}/{total_tests} tests passed ({pct:.1f}%){reset}")
    
    if passed_tests == total_tests:
        print(f"\n  {color}ğŸ‰ All tests passed! Ready for GPU testing.{reset}")
    else:
        print(f"\n  {color}âš  Some tests failed. Review before GPU testing.{reset}")


def main():
    print("\n" + "â•" * 70)
    print("  SOTA TRAINER - COMPREHENSIVE CPU TEST SUITE")
    print("  Testing all implemented components on CPU")
    print("â•" * 70)
    print(f"\n  PyTorch version: {torch.__version__}")
    print(f"  Device: CPU")
    print(f"  Test started: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Run all tests
    test_optimizers()
    test_schedulers()
    test_loss_functions()
    test_metrics()
    test_callbacks()
    test_distributed()
    test_kernels()
    test_trainer()
    test_specialized_trainers()
    
    # Print summary
    print_summary()
    
    return 0 if all(
        all(t["passed"] for t in tests.values()) 
        for tests in TEST_RESULTS.values()
    ) else 1


if __name__ == "__main__":
    sys.exit(main())
