# ════════════════════════════════════════════════════════════════════════════════
# SOTA Training Configuration - AMD ROCm MI300X Multi-GPU (FULL FINE-TUNE)
# ════════════════════════════════════════════════════════════════════════════════
# Full Fine-Tuning configuration (LoRA Disabled).
# Uses 4 GPUs.
# ════════════════════════════════════════════════════════════════════════════════

config_version: "2.0"
config_type: "sota_training"

# DataPipeline Requirements
type: "data_module"
version: "1.0"

# ═════════════════════════════════════════════════════════════════════════════════
# Training Mode - USER CONTROL
# ═════════════════════════════════════════════════════════════════════════════════
training_mode: "full"  # <--- FULL FINE-TUNING ENABLED

# ═════════════════════════════════════════════════════════════════════════════════
# Model Configuration
# ═════════════════════════════════════════════════════════════════════════════════
model:
  name_or_path: "mistralai/Mistral-7B-Instruct-v0.3"
  revision: "main"
  trust_remote_code: true
  torch_dtype: "bfloat16"
  low_cpu_mem_usage: true
  use_flash_attention_2: false
  attn_implementation: "sdpa"
  max_position_embeddings: null

# ═════════════════════════════════════════════════════════════════════════════════
# Tokenizer Configuration
# ═════════════════════════════════════════════════════════════════════════════════
tokenizer:
  name_or_path: "mistralai/Mistral-7B-Instruct-v0.3"
  use_fast: true
  padding_side: "right"
  truncation_side: "right"
  max_length: 4096
  add_bos_token: false
  add_eos_token: true
  add_pad_token: true

# ═════════════════════════════════════════════════════════════════════════════════
# Dataset Configuration
# ═════════════════════════════════════════════════════════════════════════════════
dataset:
  name: "HuggingFaceH4/ultrachat_200k"
  config_name: null
  streaming: false
  revision: null

  splits:
    train:
      name: "train_sft"
      alias: "training"
      sample_size: 1000
      shuffle: true
      seed: 42
    validation:
      name: "test_sft"
      alias: "eval"
      sample_size: 1000
      shuffle: false

  column_mapping:
    messages: "messages"

  columns: []

# ═════════════════════════════════════════════════════════════════════════════════
# Introspection Settings
# ═════════════════════════════════════════════════════════════════════════════════
introspection:
  enabled: true
  auto_discover_splits: true
  auto_discover_columns: true
  trust_remote_code: true
  cache_metadata: true
  fallback_columns:
    - "messages"
    - "conversations"

# ═════════════════════════════════════════════════════════════════════════════════
# Prompt Template Configuration
# ═════════════════════════════════════════════════════════════════════════════════
prompt_template:
  format_type: "chat"
  system_message: "You are a helpful, respectful and honest AI assistant."
  user_template: "{{ content }}"
  assistant_template: "{{ content }}"
  input_columns:
    - "messages"
  label_column: null
  mask_input: true
  add_bos: true
  add_eos: true

# ═════════════════════════════════════════════════════════════════════════════════
# Preprocessing Configuration
# ═════════════════════════════════════════════════════════════════════════════════
preprocessing:
  length_manager:
    enabled: true
    max_total_length: 4096
    padding_strategy: "longest"
    truncation_strategy: "smart"
    per_column_limits:
      instruction: 2000
      input: 2000
      output: 4000

  content_distribution:
    enabled: true
    mode: "proportional"
    column_ratios:
      instruction: 0.3
      input: 0.1
      output: 0.6
    special_tokens_budget: 10

  packing:
    enabled: true
    efficiency_threshold: 0.95
    max_sequences_per_pack: 8
    add_position_ids: true

  augmentation:
    enabled: false

# ═════════════════════════════════════════════════════════════════════════════════
# Output Schema
# ═════════════════════════════════════════════════════════════════════════════════
output_schema:
  input_ids:
    dtype: "long"
    pad_value: null
  attention_mask:
    dtype: "long"
    pad_value: 0
  labels:
    dtype: "long"
    pad_value: -100

# ═════════════════════════════════════════════════════════════════════════════════
# DataLoader Configuration
# ═════════════════════════════════════════════════════════════════════════════════
dataloader:
  batch_size: 16
  num_workers: 4
  pin_memory: true
  drop_last: false
  shuffle: true
  prefetch_factor: 2
  persistent_workers: true

# ═════════════════════════════════════════════════════════════════════════════════
# LoRA Configuration (DISABLED)
# ═════════════════════════════════════════════════════════════════════════════════
lora:
  enabled: false  # <--- DISABLED
  r: 64
  lora_alpha: 128
  lora_dropout: 0.05
  target_modules: ["q_proj", "v_proj"]
  bias: "none"
  use_rslora: true
  use_dora: false
  rank_pattern: {}

# ═════════════════════════════════════════════════════════════════════════════════
# Quantization Configuration
# ═════════════════════════════════════════════════════════════════════════════════
quantization:
  enabled: false
  load_in_4bit: true
  load_in_8bit: false
  load_in_fp8: false
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true
  llm_int8_threshold: 6.0

# ═════════════════════════════════════════════════════════════════════════════════
# Optimizer Configuration
# ═════════════════════════════════════════════════════════════════════════════════
optimizer:
  type: "adamw"             # Standard AdamW
  learning_rate: 1.0e-5     # Slightly lower LR for Full Fine-Tuning
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8
  max_grad_norm: 1.0
  percentile_clipping: 100.0
  block_wise: true

# ═════════════════════════════════════════════════════════════════════════════════
# Scheduler Configuration
# ═════════════════════════════════════════════════════════════════════════════════
scheduler:
  type: "cosine"
  warmup_steps: 0
  warmup_ratio: 0.1
  min_lr_ratio: 0.1
  stable_ratio: 0.85
  decay_type: "cosine"
  power: 1.0

# ═════════════════════════════════════════════════════════════════════════════════
# Loss Configuration
# ═════════════════════════════════════════════════════════════════════════════════
loss:
  type: "chunked_ce"
  ignore_index: -100
  label_smoothing: 0.0
  reduction: "mean"
  chunk_size: 32768
  focal_gamma: 2.0
  focal_alpha: 0.25
  dpo_beta: 0.1
  temperature: 2.0
  alpha: 0.5

# ═════════════════════════════════════════════════════════════════════════════════
# RL Configuration
# ═════════════════════════════════════════════════════════════════════════════════
rl:
  enabled: false

# ═════════════════════════════════════════════════════════════════════════════════
# Export Configuration
# ═════════════════════════════════════════════════════════════════════════════════
export:
  enabled: true
  output_dir: "./outputs/rocm_sota_full_export"
  format: "safetensors"
  push_to_hub: false
  hub_model_id: null
  hub_token: null
  hub_private: false
  merge_lora: false  # No LoRA to merge
  save_merged_16bit: true
  gguf_quantization: "q4_k_m"

# ═════════════════════════════════════════════════════════════════════════════════
# Distributed Configuration
# ═════════════════════════════════════════════════════════════════════════════════
distributed:
  enabled: true
  backend: "nccl"
  strategy: "ddp"
  world_size: 4
  gradient_checkpointing: true
  gradient_checkpointing_use_reentrant: false
  fsdp_config: {}
  deepspeed_config: null

# ═════════════════════════════════════════════════════════════════════════════════
# Hardware Configuration
# ═════════════════════════════════════════════════════════════════════════════════
hardware:
  device: "cuda"
  device_id: 0
  precision: "bf16"
  tf32: false
  compile_model: false
  compile_mode: "reduce-overhead"
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2

# ═════════════════════════════════════════════════════════════════════════════════
# Kernel Configuration
# ═════════════════════════════════════════════════════════════════════════════════
kernels:
  use_triton: true
  use_flash_attention: true
  use_fused_rope: true
  use_fused_rms_norm: true
  use_fused_cross_entropy: true
  use_fused_lora: false  # Disabled for Full FT
  use_moe_kernels: true
  autotune: true
  activation_checkpointing: true
  memory_efficient_attention: true

# ═════════════════════════════════════════════════════════════════════════════════
# Data Configuration
# ═════════════════════════════════════════════════════════════════════════════════
data:
  dataset_name: "HuggingFaceH4/ultrachat_200k"
  dataset_config: null
  streaming: false
  max_samples: 10000
  train_split: "train_sft"
  eval_split: "test_sft"
  text_column: "messages"
  max_seq_length: 4096
  packing: true
  packing_efficiency: 0.95
  shuffle: true
  seed: 42

# ═════════════════════════════════════════════════════════════════════════════════
# Training Configuration
# ═════════════════════════════════════════════════════════════════════════════════
training:
  output_dir: "./outputs/rocm_sota_full_run"
  num_train_epochs: 1
  max_steps: 500
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 4
  eval_strategy: "steps"    # Changed from 'epochs'
  eval_steps: 10
  save_strategy: "epochs"
  save_steps: 10
  save_total_limit: 3
  logging_steps: 10
  seed: 42
  resume_from_checkpoint: null
  early_stopping_patience: 3
  early_stopping_threshold: 0.01
