#!/usr/bin/env python3
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# SOTA End-to-End Demo for AMD ROCm MI300X - FSDP2 Version
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Complete FSDP2 pipeline demonstration using TrainScale SOTA modules.
# Optimized for massive scale training (Llama-3 70B+, etc.)
#
# Usage:
#   Verify Integration (CPU/Tiny Model):
#     python rocm_sota_demo_fsdp2.py --verify
#
#   Multi-GPU Training (4 GPUs):
#     torchrun --nproc_per_node=4 rocm_sota_demo_fsdp2.py --config rocm_sota_config_fsdp2.yaml
#
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

from __future__ import annotations

import argparse
import logging
import os
import sys
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Path Setup
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
_SCRIPT_DIR = Path(__file__).resolve().parent
_TRAINSCALE_ROOT = _SCRIPT_DIR.parent.parent  # TrainScale/
if str(_TRAINSCALE_ROOT) not in sys.path:
    sys.path.insert(0, str(_TRAINSCALE_ROOT))

import yaml
import torch
import torch.distributed as dist

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TrainScale SOTA Imports
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

from data_pipeline.trainer.distributed import (
    DistributedState,
    log_rank_0,
)
from data_pipeline.trainer.core.sota_config import SOTAConfig
from data_pipeline.trainer.core.sota_config import SOTAConfig
from data_pipeline.trainer.trainers.sota_trainer import create_trainer, SOTATrainer
from data_pipeline.pipeline import DataPipeline
from data_pipeline.core.types import unwrap
from data_pipeline.trainer import inference

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Logging Setup
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(name)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger("rocm_sota_demo_fsdp2")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Configuration Loader
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class SOTAFSDP2Config:
    """SOTA FSDP2 configuration container from YAML."""
    raw: Dict[str, Any] = field(default_factory=dict)
    config_path: str = ""
    
    @classmethod
    def from_yaml(cls, path: str) -> "SOTAFSDP2Config":
        """Load configuration from YAML file."""
        p = Path(path)
        if not p.exists():
            # Try relative to script dir
            p = _SCRIPT_DIR / path
        if not p.exists():
            raise FileNotFoundError(f"Config not found: {path}")
        
        with open(p, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
        
        inst = cls(raw=data, config_path=str(p))
        return inst
    
    @property
    def model(self) -> Dict[str, Any]:
        return self.raw.get("model", {})
    
    @property
    def distributed(self) -> Dict[str, Any]:
        return self.raw.get("distributed", {})
    
    @property
    def optimizer(self) -> Dict[str, Any]:
        return self.raw.get("optimizer", {})
    
    @property
    def scheduler(self) -> Dict[str, Any]:
        return self.raw.get("scheduler", {})
    
    @property
    def training(self) -> Dict[str, Any]:
        return self.raw.get("training", {})
    
    @property
    def dataloader(self) -> Dict[str, Any]:
        return self.raw.get("dataloader", {})
    
    @property
    def export(self) -> Dict[str, Any]:
        return self.raw.get("export", {})


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# SOTA FSDP2 Demo Class
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class SOTADemo:
    """
    SOTA FSDP2 Demo using SOTATrainer and DataPipeline.
    """
    config_path: str
    
    def __init__(self, config_path: str):
        self.config_path = config_path
        self._data_pipeline = None
        
        # Initialize Distributed State
        self.dist_state = DistributedState.initialize().unwrap()
        
        log_rank_0(
            f"SOTA FSDP2 Demo initialized: "
            f"rank={self.dist_state.rank}/{self.dist_state.world_size}",
        )

    def verify_integration(self):
        """Run deep integration verification checks (CPU/Tiny Model friendly)."""
        log_rank_0("üß™ Starting FSDP2 Deep Integration Verification...")
        
        # 1. Config Check
        # 1. Config Check
        # Using SOTAConfig to load first (handles path resolution)
        self.config = SOTAFSDP2Config.from_yaml(self.config_path)
        
        if self.config.distributed.get("strategy") != "fsdp2":
             # Auto-fix for verification if config points elsewhere
             log_rank_0(f"‚ö†Ô∏è Config strategy is '{self.config.distributed.get('strategy')}', forcing 'fsdp2' for verification.")
             self.config.distributed["strategy"] = "fsdp2"

        log_rank_0("‚úÖ Config: Strategy check passed")
        
        # Override for verification: always use tiny model + conservative kernels.
        if self.dist_state.world_size == 1:
            log_rank_0("   Modes: Single-Process Verification...")
        else:
            log_rank_0("   Modes: Multi-Process Verification...")

        log_rank_0("   Using tiny random model for verification...")
        self.config.model["name_or_path"] = "HuggingFaceM4/tiny-random-LlamaForCausalLM"
        self.config.model["torch_dtype"] = "float32"
        self.config.quantization = self.config.raw.setdefault("quantization", {})
        self.config.quantization["enabled"] = False

        # Keep verification deterministic/stable.
        # IMPORTANT: kernel patching is controlled by top-level `kernels.*`,
        # not only distributed fsdp_config. Force all Triton/fused paths off
        # for verify mode to avoid ROCm Triton pointer faults.
        kernel_cfg = self.config.raw.setdefault("kernels", {})
        if isinstance(kernel_cfg, dict):
            kernel_cfg["use_triton"] = False
            kernel_cfg["use_flash_attention"] = False
            kernel_cfg["use_fused_rms_norm"] = False
            kernel_cfg["use_fused_rope"] = False
            kernel_cfg["use_fused_cross_entropy"] = False
            kernel_cfg["use_fused_lora"] = False
            kernel_cfg["use_moe_kernels"] = False
            self.config.raw["kernels"] = kernel_cfg

        fsdp_cfg = self.config.distributed.get("fsdp_config", {})
        if isinstance(fsdp_cfg, dict):
            fsdp_cfg["use_triton_kernels"] = False
            fsdp_cfg["activation_checkpointing"] = False
            self.config.distributed["fsdp_config"] = fsdp_cfg

        # Force device to CPU if no GPU.
        if not torch.cuda.is_available():
            self.config.raw.setdefault("hardware", {})["device"] = "cpu"
            log_rank_0("   ‚ö†Ô∏è CUDA not available, falling back to CPU for verification.")

        # 2. Model Setup
        log_rank_0("   Setting up model...")
        # Create trainer AFTER overrides
        trainer_config = SOTAConfig.from_dict(self.config.raw)
        trainer = SOTATrainer(trainer_config)
        trainer.setup_model()
        
        log_rank_0(f"‚úÖ Model: Setup complete (Type: {type(trainer.model).__name__})")

        # 3. Data Check (Real Data via DataPipeline)
        log_rank_0("   Creating Real DataLoader from DataPipeline...")
        dp_result = DataPipeline.from_config(
            self.config_path,
            token=os.environ.get("HF_TOKEN"),
            trust_remote_code=False,
        )
        self._data_pipeline = unwrap(dp_result)

        dl_result = self._data_pipeline.get_dataloader(
            split=trainer.config.data.train_split,
            batch_size=trainer.config.training.per_device_train_batch_size,
            distributed=(self.dist_state.world_size > 1),
            rank=self.dist_state.rank,
            world_size=self.dist_state.world_size,
        )
        dataloader = unwrap(dl_result)
        log_rank_0("‚úÖ Data: Real DataLoader created")
        
        # 4. Runtime Check
        if self.dist_state.world_size > 1:
            # NOTE:
            # On some ROCm stacks, tiny-model FSDP2 backward/step in verify mode
            # can trigger low-level GPU hangs (agent memory faults). For verify we
            # keep multi-rank checks to a synchronized forward-only smoke test.
            log_rank_0(
                "   Running multi-rank forward-only smoke check "
                "(skip backward/optimizer in verify mode)..."
            )
            try:
                batch = next(iter(dataloader))
                batch = {
                    k: v.to(trainer.device)
                    if isinstance(v, torch.Tensor) else v
                    for k, v in batch.items()
                }
                trainer.model.eval()
                with torch.no_grad():
                    if getattr(trainer, "_is_fsdp2_active", False):
                        with trainer._fsdp2_engine.forward_context():
                            outputs = trainer.model(
                                input_ids=batch["input_ids"],
                                attention_mask=batch.get("attention_mask"),
                                labels=batch.get("labels"),
                            )
                    else:
                        outputs = trainer.model(
                            input_ids=batch["input_ids"],
                            attention_mask=batch.get("attention_mask"),
                            labels=batch.get("labels"),
                        )

                if hasattr(outputs, "loss") and outputs.loss is not None:
                    log_rank_0(
                        f"‚úÖ Forward smoke loss: {outputs.loss.item():.4f}"
                    )
                else:
                    log_rank_0("‚úÖ Forward smoke completed.")

                if dist.is_initialized():
                    dist.barrier()
            except Exception as e:
                log_rank_0(f"‚ùå Forward smoke check failed: {e}")
                raise
            finally:
                trainer.model.train()
        else:
            log_rank_0("   Running 1 Step Training Loop...")
            trainer.config.training.max_steps = 1
            trainer.config.training.logging_steps = 1
            trainer.train(dataloader)
            log_rank_0("‚úÖ Training: 1 step completed successfully")
        
        log_rank_0("üéâ All FSDP2 Integration Checks Passed!")

    def _export_model(self, trainer):
        """Export trained model."""
        export_cfg = trainer.config.export
        if not getattr(export_cfg, "enabled", False):
            return

        log_rank_0("Exporting Model...")
        try:
             trainer.export(tokenizer=self._data_pipeline._tokenizer_wrapper.tokenizer)
        except Exception as e:
             log_rank_0(f"‚ö†Ô∏è Export failed: {e}")

    def _run_inference(self, trainer):
        """Benchmark SOTA Inference Engine."""
        log_rank_0("\nüöÄ Step 9: Benchmarking SOTA Inference...")
        
        export_cfg = trainer.config.export
        export_dir = getattr(export_cfg, "output_dir", None)
        if not export_dir or not os.path.exists(export_dir):
             log_rank_0(f"‚ö†Ô∏è Export directory {export_dir} not found. Skipping inference benchmark.")
             return
        
        try:
            dtype = "bfloat16" if torch.cuda.is_available() else "float32"
            engine_cfg = inference.EngineConfig(
                model=export_dir,
                tokenizer=export_dir,
                dtype=dtype,
                trust_remote_code=True,
            )
            engine = inference.SOTAInferenceEngine(engine_cfg)

            gen_params = inference.GenerationParams(
                max_tokens=30,
                temperature=0.7,
                top_p=0.95,
            )

            prompts = [
                "Explain FSDP2 in 2 lines.",
                "What is ROCm in 2 lines?",
            ]

            result = engine.generate(prompts, params=gen_params)
            if result.is_err():
                log_rank_0(f"‚ùå Inference engine error: {result.error}")
                return

            responses = result.unwrap()
            for i, resp in enumerate(responses):
                text = getattr(resp, "generated_text", "")
                log_rank_0(f"Response {i}: {text[:80]}...")

        except Exception as e:
            log_rank_0(
                f"‚ö†Ô∏è SOTAInferenceEngine failed ({e}); "
                "falling back to Transformers generate..."
            )
            try:
                from transformers import AutoModelForCausalLM, AutoTokenizer

                fallback_tokenizer = AutoTokenizer.from_pretrained(export_dir)
                fallback_model = AutoModelForCausalLM.from_pretrained(
                    export_dir,
                    torch_dtype=(
                        torch.bfloat16
                        if torch.cuda.is_available()
                        else torch.float32
                    ),
                    device_map="auto" if torch.cuda.is_available() else "cpu",
                    trust_remote_code=True,
                )

                prompts = [
                    "Explain FSDP2 in 2 lines.",
                    "What is ROCm in 2 lines?",
                ]
                for i, prompt in enumerate(prompts):
                    inputs = fallback_tokenizer(
                        prompt,
                        return_tensors="pt",
                    )
                    if torch.cuda.is_available():
                        inputs = {
                            k: v.to(fallback_model.device)
                            for k, v in inputs.items()
                        }
                    with torch.no_grad():
                        output_ids = fallback_model.generate(
                            **inputs,
                            max_new_tokens=30,
                            do_sample=False,
                        )
                    text = fallback_tokenizer.decode(
                        output_ids[0], skip_special_tokens=True
                    )
                    log_rank_0(f"Fallback Response {i}: {text[:80]}...")
            except Exception as fallback_err:
                log_rank_0(f"‚ùå Inference benchmark failed: {fallback_err}")


    def run(self, max_steps: int = 100):
        """Run the demo."""
        log_rank_0("‚ïê" * 60)
        log_rank_0("Starting SOTA FSDP2 Training")
        log_rank_0("‚ïê" * 60)
        
        # 1. Initialize Data Pipeline
        log_rank_0("Initializing Data Pipeline...")
        dp_result = DataPipeline.from_config(
            self.config_path,
            token=os.environ.get("HF_TOKEN"),
            trust_remote_code=False,
        )
        self._data_pipeline = unwrap(dp_result)
        
        # 2. Create SOTA Trainer
        log_rank_0("Initializing SOTA Trainer...")
        self.config = SOTAFSDP2Config.from_yaml(self.config_path) # Cache config
        trainer_config = SOTAConfig.from_dict(self.config.raw)
        trainer = SOTATrainer(trainer_config)
        
        # Setup Model
        log_rank_0("Setting up SOTA FSDP2 Model...")
        trainer.setup_model()
        
        if max_steps > 0:
            trainer.config.training.max_steps = max_steps
        
        # 3. DataLoader
        log_rank_0("Creating Distributed DataLoader...")
        dl_result = self._data_pipeline.get_dataloader(
            split=trainer.config.data.train_split,
            batch_size=trainer.config.training.per_device_train_batch_size,
            distributed=(self.dist_state.world_size > 1),
            rank=self.dist_state.rank,
            world_size=self.dist_state.world_size,
        )
        dataloader = unwrap(dl_result)
        
        # Eval DataLoader
        eval_dl_result = self._data_pipeline.get_dataloader(
            split=trainer.config.data.eval_split,
            batch_size=trainer.config.training.per_device_eval_batch_size,
            distributed=(self.dist_state.world_size > 1),
            rank=self.dist_state.rank,
            world_size=self.dist_state.world_size,
        )
        eval_dataloader = unwrap(eval_dl_result)
        
        # 4. Train
        log_rank_0("Starting Training Loop...")
        metrics = trainer.train(dataloader, eval_dataloader=eval_dataloader)
        
        log_rank_0("Training Complete!")
        log_rank_0(f"Final Metrics: {metrics}")
        
        # 5. Export
        # IMPORTANT: FSDP2 export uses collective full-parameter gathering.
        # All ranks must call trainer.export(); only rank 0 writes files.
        if trainer.config.export.enabled:
            self._export_model(trainer)

            # 6. Benchmark Inference (rank 0 only)
            if self.dist_state.rank == 0:
                self._run_inference(trainer)

            # Keep non-zero ranks alive until rank 0 finishes inference/export.
            if dist.is_initialized():
                dist.barrier()
        
        return metrics


def main():
    parser = argparse.ArgumentParser(description="ROCm SOTA FSDP2 Demo")
    parser.add_argument("--config", type=str, default="rocm_sota_config_fsdp2.yaml", help="Path to YAML config")
    parser.add_argument("--verify", action="store_true", help="Run integration verification (Tiny Model)")
    parser.add_argument("--max-steps", type=int, default=-1, help="Max training steps")
    args = parser.parse_args()
    
    # Environment Setup
    if "RANK" not in os.environ:
        os.environ["RANK"] = "0"
        os.environ["LOCAL_RANK"] = "0"
        os.environ["WORLD_SIZE"] = "1"
        os.environ["MASTER_ADDR"] = "localhost"
        os.environ["MASTER_PORT"] = "29500"
    
    demo: Optional[SOTADemo] = None
    try:
        demo = SOTADemo(args.config)
        
        if args.verify:
            demo.verify_integration()
        else:
            demo.run(max_steps=args.max_steps)
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        # Ensure all distributed resources are released to avoid
        # ProcessGroup warnings when the script exits.
        if demo is not None and hasattr(demo, "dist_state"):
            demo.dist_state.shutdown()
        elif dist.is_initialized():
            dist.destroy_process_group()

if __name__ == "__main__":
    main()
