# ════════════════════════════════════════════════════════════════════════════════
# SOTA Training Configuration - Zero Bubble Pipeline Parallelism (ZBPP)
# ════════════════════════════════════════════════════════════════════════════════
# Optimized for pipeline-parallel training with zero bubbles.
#
# Hardware: Multi-GPU (e.g., 4x MI300X or H100)
# Strategy: "pipeline_zbpp"
# ════════════════════════════════════════════════════════════════════════════════

config_version: "2.0"
config_type: "sota_training"

# DataPipeline Requirements
type: "data_module"
version: "1.0"

training_mode: "full"

# Quantization (Disabled for ZBPP Demo)
quantization:
  enabled: false

# ═════════════════════════════════════════════════════════════════════════════════
# Model Configuration
# ═════════════════════════════════════════════════════════════════════════════════
model:
  name_or_path: "mistralai/Mistral-7B-Instruct-v0.3"
  revision: "main"
  trust_remote_code: true
  torch_dtype: "bfloat16"
  low_cpu_mem_usage: true
  use_flash_attention_2: false
  attn_implementation: "sdpa"

# ═════════════════════════════════════════════════════════════════════════════════
# Tokenizer Configuration
# ═════════════════════════════════════════════════════════════════════════════════
tokenizer:
  name_or_path: "mistralai/Mistral-7B-Instruct-v0.3"
  use_fast: true
  padding_side: "right"
  truncation_side: "right"
  max_length: 4096
  add_bos_token: false
  add_eos_token: true
  add_pad_token: true

# ═════════════════════════════════════════════════════════════════════════════════
# Dataset Configuration
# ═════════════════════════════════════════════════════════════════════════════════
dataset:
  name: "HuggingFaceH4/ultrachat_200k"
  splits:
    train:
      name: "train_sft"
      sample_size: 10000
    validation:
      name: "test_sft"
      sample_size: 1000
  column_mapping:
    messages: "messages"
  columns: []

introspection:
  enabled: true
  auto_discover_splits: true
  auto_discover_columns: true
  fallback_columns: ["messages", "conversations"]

# ═════════════════════════════════════════════════════════════════════════════════
# Prompt Configuration
# ═════════════════════════════════════════════════════════════════════════════════
prompt_template:
  format_type: "chat"
  input_columns: ["messages"]
  mask_input: true

# ═════════════════════════════════════════════════════════════════════════════════
# Preprocessing Configuration
# ═════════════════════════════════════════════════════════════════════════════════
preprocessing:
  length_manager:
    enabled: true
    max_total_length: 4096
    padding_strategy: "longest"
    truncation_strategy: "smart"
  content_distribution:
    enabled: true
    mode: "proportional"
  packing:
    enabled: true

# ═════════════════════════════════════════════════════════════════════════════════
# Output Schema
# ═════════════════════════════════════════════════════════════════════════════════
output_schema:
  input_ids: {dtype: "long", pad_value: null}
  attention_mask: {dtype: "long", pad_value: 0}
  labels: {dtype: "long", pad_value: -100}

# ═════════════════════════════════════════════════════════════════════════════════
# DataLoader
# ═════════════════════════════════════════════════════════════════════════════════
dataloader:
  batch_size: 32  # Global batch size per step will be split into microbatches
  num_workers: 4
  pin_memory: true
  persistent_workers: true

# ═════════════════════════════════════════════════════════════════════════════════
# Optimizer & Scheduler
# ═════════════════════════════════════════════════════════════════════════════════
optimizer:
  type: "adamw"
  learning_rate: 1.0e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8

scheduler:
  type: "cosine"
  warmup_steps: 20

loss:
  type: "cross_entropy"

# ═════════════════════════════════════════════════════════════════════════════════
# Distributed Configuration - ZBPP
# ═════════════════════════════════════════════════════════════════════════════════
distributed:
  enabled: true
  backend: "nccl"
  strategy: "pipeline_zbpp"
  world_size: 4
  
  # ZBPP Specific Settings
  num_pipeline_stages: 4
  num_microbatches: 8
  pipeline_memory_limit_gb: 4.0

# ═════════════════════════════════════════════════════════════════════════════════
# Hardware
# ═════════════════════════════════════════════════════════════════════════════════
hardware:
  device: "cuda"
  precision: "bf16"

# ═════════════════════════════════════════════════════════════════════════════════
# Kernels
# ═════════════════════════════════════════════════════════════════════════════════
kernels:
  use_triton: true
  use_flash_attention: true

# ═════════════════════════════════════════════════════════════════════════════════
# Training
# ═════════════════════════════════════════════════════════════════════════════════
training:
  output_dir: "./outputs/zbpp_run"
  num_train_epochs: 1
  per_device_train_batch_size: 32 # This acts as the total batch size to be split into microbatches for ZBPP
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 1  # ZBPP handles its own accumulation via microbatches
  eval_strategy: "steps"
  eval_steps: 10
  save_strategy: "steps"
  save_steps: 50
  logging_steps: 1
  max_steps: 100

# ═════════════════════════════════════════════════════════════════════════════════
# Export
# ═════════════════════════════════════════════════════════════════════════════════
export:
  enabled: true
  output_dir: "./outputs/zbpp_export"
  merge_lora: false # ZBPP usually full fine-tune, but keep for consistency
