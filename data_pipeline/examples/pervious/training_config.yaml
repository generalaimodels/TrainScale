# ════════════════════════════════════════════════════════════════════════════════
# Training Configuration - SOTA Trainer
# ════════════════════════════════════════════════════════════════════════════════
# User-controlled training configuration for SOTA pipeline
# All parameters are configured here - no hardcoding in code
# ════════════════════════════════════════════════════════════════════════════════

type: training_config
version: "1.0"

# ─────────────────────────────────────────────────────────────────────────────────
# Model Configuration
# ─────────────────────────────────────────────────────────────────────────────────
model:
  name_or_path: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # 1.1B params, fits in Colab GPU
  trust_remote_code: false
  # Loading options
  torch_dtype: "bfloat16"  # Use bf16 for efficiency (fp16 on T4)
  low_cpu_mem_usage: true
  
# ─────────────────────────────────────────────────────────────────────────────────
# Tokenizer Configuration
# ─────────────────────────────────────────────────────────────────────────────────
tokenizer:
  name_or_path: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # Use same as model
  use_fast: true
  padding_side: "right"
  # TinyLlama has proper pad token
  add_pad_token: false

# ─────────────────────────────────────────────────────────────────────────────────
# Training Arguments
# ─────────────────────────────────────────────────────────────────────────────────
training:
  # Output
  output_dir: "./outputs/sota_trainer_test"
  overwrite_output_dir: true
  
  # Training duration
  num_train_epochs: 2
  max_steps: -1  # -1 means use epochs
  
  # Batch sizes
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 2
  
  # Learning rate
  learning_rate: 5.0e-5
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Warmup
  warmup_ratio: 0.1
  warmup_steps: 0  # If 0, use warmup_ratio
  
  # Scheduler
  lr_scheduler_type: "cosine"  # cosine, linear, polynomial
  
  # Optimizer
  optim: "adamw"  # adamw, lamb, adafactor

# ─────────────────────────────────────────────────────────────────────────────────
# Evaluation Configuration
# ─────────────────────────────────────────────────────────────────────────────────
evaluation:
  eval_strategy: "steps"  # "steps", "epoch", "no"
  eval_steps: 10
  eval_delay: 0
  
  # Metrics to compute
  metrics:
    - "loss"
    - "perplexity"

# ─────────────────────────────────────────────────────────────────────────────────
# Saving Configuration
# ─────────────────────────────────────────────────────────────────────────────────
saving:
  save_strategy: "steps"  # "steps", "epoch", "no"
  save_steps: 25
  save_total_limit: 2
  save_safetensors: true
  
  # Best model selection
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

# ─────────────────────────────────────────────────────────────────────────────────
# Logging Configuration
# ─────────────────────────────────────────────────────────────────────────────────
logging:
  logging_steps: 5
  logging_first_step: true
  log_level: "info"
  
  # Backends (console always enabled)
  report_to: []  # "tensorboard", "wandb"

# ─────────────────────────────────────────────────────────────────────────────────
# Hardware Configuration
# ─────────────────────────────────────────────────────────────────────────────────
hardware:
  # Device selection
  device: "auto"  # "auto" selects GPU if available
  
  # Precision - Enable AMP for faster training
  fp16: true  # Use fp16 on T4/V100 (Colab default)
  bf16: false  # Set true if on A100/H100
  tf32: true  # Enable on Ampere+ GPUs
  
  # Multi-GPU
  dataloader_num_workers: 2
  dataloader_pin_memory: true  # Faster GPU transfer

# ─────────────────────────────────────────────────────────────────────────────────
# Callbacks
# ─────────────────────────────────────────────────────────────────────────────────
callbacks:
  early_stopping:
    enabled: false
    patience: 3
    threshold: 0.001
  
  progress:
    enabled: true
    show_eta: true
