# ════════════════════════════════════════════════════════════════════════════════
# Complete End-to-End Pipeline Configuration
# ════════════════════════════════════════════════════════════════════════════════
# Single YAML file controlling the entire pipeline:
#   - Dataset loading & introspection
#   - Column mapping & discovery
#   - Preprocessing & tokenization
#   - DataLoader configuration
#   - Training configuration (SOTA)
#
# ALL settings are here - NO hardcoding anywhere in the code.
# ════════════════════════════════════════════════════════════════════════════════

# ═════════════════════════════════════════════════════════════════════════════════
# Pipeline Meta
# ═════════════════════════════════════════════════════════════════════════════════
type: "data_module"
version: "2.0"

# ═════════════════════════════════════════════════════════════════════════════════
# Dataset Configuration
# ═════════════════════════════════════════════════════════════════════════════════
# - All splits discovered and configured here
# - Column mapping from source to target
# - User controls everything
# ═════════════════════════════════════════════════════════════════════════════════
dataset:
  name: "tatsu-lab/alpaca"
  config_name: null  # Optional HF config/subset
  streaming: false   # Set true for large datasets
  revision: null     # Optional git revision

  # Split Configuration (discovered by introspection, user confirms)
  splits:
    train:
      name: "train"           # HF split name
      alias: "training"       # User-facing alias
      sample_size: null       # null = all, or number for subset
      shuffle: true
      seed: 42
    # Additional splits (uncomment if available)
    # validation:
    #   name: "validation"
    #   alias: "eval"
    #   shuffle: false

  # Column Mapping: source_column -> target_column
  # Maps dataset columns to standard names used by the pipeline
  column_mapping:
    instruction: "instruction"
    input: "input"
    output: "output"
    text: "text"

  # Columns to keep (empty = keep all discovered columns)
  columns: []

# ═════════════════════════════════════════════════════════════════════════════════
# Introspection Settings
# ═════════════════════════════════════════════════════════════════════════════════
# Controls automatic discovery of dataset structure
# ═════════════════════════════════════════════════════════════════════════════════
introspection:
  enabled: true
  auto_discover_splits: true
  auto_discover_columns: true
  trust_remote_code: false
  cache_metadata: true
  # Fallback columns if introspection fails
  fallback_columns:
    - "instruction"
    - "input"
    - "output"
    - "text"

# ═════════════════════════════════════════════════════════════════════════════════
# Tokenizer Configuration
# ═════════════════════════════════════════════════════════════════════════════════
tokenizer:
  name_or_path: "meta-llama/Llama-3.1-8B-Instruct"
  max_length: 4096
  padding: "max_length"      # max_length, longest, do_not_pad
  truncation: true
  padding_side: "right"      # left, right
  truncation_side: "right"
  add_special_tokens: true
  
  # Special tokens (use tokenizer defaults if empty)
  special_tokens:
    pad_token: "<|finetune_right_pad_id|>"
    # eos_token: "<|eot_id|>"
    # bos_token: "<|begin_of_text|>"

# ═════════════════════════════════════════════════════════════════════════════════
# Prompt Template Configuration
# ═════════════════════════════════════════════════════════════════════════════════
# Supports: chat, completion, custom (Jinja2)
# ═════════════════════════════════════════════════════════════════════════════════
prompt_template:
  format_type: "chat"  # chat, completion, custom

  # Chat format configuration
  system_message: "You are a helpful AI assistant."
  user_template: "{{ instruction }}{% if input %}\n\nInput: {{ input }}{% endif %}"
  assistant_template: "{{ output }}"

  # Column roles
  input_columns:
    - "instruction"
    - "input"
  label_column: "output"
  
  # Label masking (mask input portion from loss)
  mask_input: true
  add_bos: true
  add_eos: true

  # Custom template (only used if format_type: custom)
  # template: |
  #   ### Instruction:
  #   {{ instruction }}
  #   {% if input %}
  #   ### Input:
  #   {{ input }}
  #   {% endif %}
  #   ### Response:
  #   {{ output }}

# ═════════════════════════════════════════════════════════════════════════════════
# Preprocessing Configuration (SOTA)
# ═════════════════════════════════════════════════════════════════════════════════
# Features:
# - Token-aware content distribution across columns
# - Smart truncation (sentence/word boundaries)
# - Per-column limits with priority-based trimming
# - Sequence packing for efficiency
# ═════════════════════════════════════════════════════════════════════════════════
preprocessing:
  # ─────────────────────────────────────────────────────────────────────────────
  # Length Manager: Per-column limits and truncation
  # ─────────────────────────────────────────────────────────────────────────────
  length_manager:
    enabled: true
    max_total_length: 4096        # Maximum sequence length
    padding_strategy: "longest"   # longest, max_length, do_not_pad, bucket
    truncation_strategy: "smart"  # smart, simple, word_boundary, sentence_boundary
    
    # Per-column character limits (optional)
    per_column_limits:
      instruction: 4000   # Max chars for instruction column
      input: 4000         # Max chars for input column
      output: 8000        # Max chars for output column

  # ─────────────────────────────────────────────────────────────────────────────
  # Content Distribution: SOTA token-aware allocation (advanced)
  # ─────────────────────────────────────────────────────────────────────────────
  content_distribution:
    enabled: true
    mode: "proportional"  # equal, proportional, ratio, priority, adaptive
    
    # Column ratios for ratio mode (must sum to <= 1.0)
    column_ratios:
      instruction: 0.3
      input: 0.1
      output: 0.55
    
    # Reserved tokens for BOS/EOS/special tokens
    special_tokens_budget: 10

  # ─────────────────────────────────────────────────────────────────────────────
  # Packing: Combine short sequences for efficiency
  # ─────────────────────────────────────────────────────────────────────────────
  packing:
    enabled: true
    efficiency_threshold: 0.95
    max_sequences_per_pack: 8
    add_position_ids: true

  # ─────────────────────────────────────────────────────────────────────────────
  # Data augmentation (optional)
  # ─────────────────────────────────────────────────────────────────────────────
  augmentation:
    enabled: false
    # shuffle_sentences: false
    # random_case: false

# ═════════════════════════════════════════════════════════════════════════════════
# Output Schema (for DataLoader batches)
# ═════════════════════════════════════════════════════════════════════════════════
# Ensures tensors are loss-aligned and ready for model.forward()
# ═════════════════════════════════════════════════════════════════════════════════
output_schema:
  input_ids:
    dtype: "long"
    pad_value: null  # Uses tokenizer pad_token_id
  attention_mask:
    dtype: "long"
    pad_value: 0
  labels:
    dtype: "long"
    pad_value: -100  # Ignored by CrossEntropyLoss

# ═════════════════════════════════════════════════════════════════════════════════
# DataLoader Configuration
# ═════════════════════════════════════════════════════════════════════════════════
dataloader:
  batch_size: 4
  num_workers: 4
  pin_memory: true
  drop_last: false
  shuffle: true
  prefetch_factor: 2
  persistent_workers: true

# ═════════════════════════════════════════════════════════════════════════════════
# Training Configuration (SOTA - Above Unsloth)
# ═════════════════════════════════════════════════════════════════════════════════
training:
  # Training mode
  mode: "qlora"  # full, lora, qlora, pretrain, rl
  
  # Model
  model:
    name_or_path: "meta-llama/Llama-3.1-8B-Instruct"
    use_flash_attention_2: true
    attn_implementation: "flash_attention_2"
  
  # LoRA configuration
  lora:
    enabled: true
    r: 16
    lora_alpha: 32
    lora_dropout: 0.0
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"
    use_rslora: true
    use_dora: false
  
  # Quantization (for QLoRA)
  quantization:
    enabled: true
    load_in_4bit: true
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_compute_dtype: "bfloat16"
    bnb_4bit_use_double_quant: true
  
  # Optimizer
  optimizer:
    type: "adamw"  # adamw, adam8bit, lion, came, sophia, prodigy
    learning_rate: 2.0e-5
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 1.0e-8
    max_grad_norm: 1.0
  
  # Scheduler
  scheduler:
    type: "cosine"  # cosine, wsd, linear, polynomial, onecycle
    warmup_ratio: 0.1
    min_lr_ratio: 0.1
  
  # Loss
  loss:
    type: "cross_entropy"  # cross_entropy, chunked_ce, focal
    ignore_index: -100
    label_smoothing: 0.0
  
  # Training loop
  num_epochs: 3
  max_steps: -1
  gradient_accumulation_steps: 4
  
  # Evaluation
  eval_strategy: "steps"
  eval_steps: 100
  
  # Saving
  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 3
  output_dir: "./outputs"
  
  # Logging
  logging_steps: 10
  
  # Hardware
  hardware:
    device: "auto"
    precision: "bf16"
    tf32: true
    compile_model: false
  
  # Kernels
  kernels:
    use_triton: true
    use_flash_attention: true
    use_fused_cross_entropy: true
    activation_checkpointing: true
  
  # Seed
  seed: 42

# ═════════════════════════════════════════════════════════════════════════════════
# Export Configuration
# ═════════════════════════════════════════════════════════════════════════════════
export:
  enabled: false
  output_dir: "./exported_model"
  format: "safetensors"  # safetensors, gguf_q4_k_m, gguf_q8_0, vllm
  merge_lora: true
  push_to_hub: false
  hub_model_id: null
